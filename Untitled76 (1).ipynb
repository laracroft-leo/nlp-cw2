{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2116,
     "status": "ok",
     "timestamp": 1743747989025,
     "user": {
      "displayName": "yu Wu",
      "userId": "12692660435918028293"
     },
     "user_tz": -60
    },
    "id": "tGo01wvweQDA",
    "outputId": "ac744039-88f7-49ae-8cff-6e55064cc6cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji==0.6.0 in /usr/local/lib/python3.11/dist-packages (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install emoji==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6ebb70cb104f4aa5aa0faf13f2e2ee3b",
      "1f0bfa93d3c0456ea4cf61a30abc2991",
      "271dd8b8c41243048368047a1ac78077",
      "b701806265a3448ea0a4c35efc58f061",
      "101b4740e1a54a71aababddf6c574cb1",
      "ee26f55e3f454f21a20f49ab7d59b236",
      "8d83db45fded496588eac219fcf43385",
      "8c24db8696a44cfe81643cfe9c906bb1",
      "4da22654e302440196e30d043443f09f",
      "f445daf4f35c4a5a8a7a70574b11d384",
      "62d1ad4a92a8445e8f611e7ad05ac771",
      "8bdff2e6f61b42b0b860fb00d25c66bf",
      "b22282c9d9a1459ea3839c6d8d92d2e0",
      "bd4f3c578d644d00a832a1936b8ba68d",
      "97da6ac5442843e687dc2272b8002577",
      "a7b3f65abd99478190fceae2c07a813a",
      "e40cdaf6203345d8ae56b4a15e52c6e6",
      "ba20c6c4cce64c95a2e0f10c49eaeb95",
      "9a8f4afcfd994622bc1ac29876bdaa8b",
      "aed6b3ae39c940988caf596a1b91afcc",
      "387d52e6b5594d1f98ca44e491ff4da8",
      "705932f7f30e4bab85e0d41ac04591d1",
      "5693c382a12c4b5da9b009c18202e91f",
      "a3d9fd3cea944172b81f2db6494ba077",
      "57051bcd5382446b8975bacdd095dc73",
      "1155b21d9d9a4563b58cb342a8aaf0c0",
      "35bc1ad0610b486db4e73437bd9a79f7",
      "16fa88893f5b4c66a4df00428e7ac46f",
      "8dc99ec9c6ad485bad4b83cb6b19f5e4",
      "fbdf794befbf4673af2917ce36817632",
      "69398dc269b14e55af2f1ef687e7ebda",
      "337e99d9044b4127b6a264a5d5e883b2",
      "dbf1633077d946f0b33775ba7668eff0",
      "a86c3a65304f4271bb9364c80de37270",
      "6a861a71f5d04b7d88cf6bddd2605925",
      "ca82f730cf3e4ff39f0f147b92089568",
      "f929a68e136f490980af7131ca185c80",
      "78d8ad090f21473ba07d6ffda2b053c0",
      "34aad9f26d1d480bb2f0a4b913b307ce",
      "1aecfbf73bb3477582f16ec514fd8c1a",
      "dadaf4c9f1174ea6938ee53622bb71f7",
      "5b153e79750143b496ec53783d3ff7d1",
      "d3191139167d4a1ebb989100ef98dd7b",
      "0a6aede3aa7e4741a8802cbcbb54f28f",
      "9f1ef2851f69471d9b9486e724e6f656",
      "097f6600c8b74002a647723c3ef4aed2",
      "6dcb8fff1b654e9ca49e39c433e1cb23",
      "6ed7a78582344778bdcc8920944b5ed1",
      "48033cd1cdeb460399b302bd98f6cbca",
      "b9e8a8a398664193bd81b5abc98d4259",
      "7b2b2722b1084ab4b8924b4bfd9df974",
      "bff0ce9897384f94b57cecb35c375bc7",
      "acca6c7f23ce454e808286a894f83390",
      "85016266b95849d397be536fb03b5184",
      "274fbd5f824449dba2591412c3190430",
      "e0eb621b179b4d88b015e89589878d11",
      "9cdbe9d705bc4f779171fe625e803624",
      "113d3734f3f649c1a7d78cf52ea5f71e",
      "219def18d8794d04a1c9b7e23f7a1e14",
      "8993acf7d3744460ab8cf242674ac32c",
      "5c274a7e045f4e379d85264f52ac164c",
      "8909252af28b42c08243d6bc6f07673f",
      "016529968ce048268936bedc618de95d",
      "3e676ee027b84e24a894fa74fd003587",
      "064bec9c2d834fff994febfd15e83a48",
      "46aee757d0fa4ed78f6207e20537f143",
      "7b7d1df1005a4119aaa194131c2c876e",
      "2e7fc62a282c45dd909038dcac6c1ba5",
      "b2c84eb3be19431694826ab375b889e8",
      "d60b00e9973f497a9a4ef43c84b2749a",
      "e13729132077428c82b27e2cb4237cf6",
      "2836b9dfbf194465af9cbdf83602d013",
      "88d93df11a3a41a48c45c65044a31896",
      "f842511df3e54dc3b39ae2c985d07e7c",
      "6654812b52b54fa7969b41d27b09a8ad",
      "d2965e77d9ec4666a9768cba20c683ba",
      "8a6d2f24834b4af9a290adfec6843b50",
      "1bf8f71b22f54633bc491f5f7ca7376e",
      "04e0dc338bce4aa08894f823bcb12b70",
      "bbef1536504b4196b708554462d6f1b6",
      "01be554be0484c52bc1d10187eef0ba7",
      "294bf3de283c40e184f6adaa43db5639",
      "535c5701b6b04912bea4bc74d9eaa2a2",
      "35aedf7ee0ff453c8d1ee66ec3857cdb",
      "682654d92b2649cdaca8c16fb66bcbef",
      "606d009e10114306a40e97ad8843facf",
      "8edb907d8fe943cea8e5859ac3fea374",
      "00737d052e86488a9c7962204514cc2c",
      "5038868f27e943bebbd29a4cd72af0f0",
      "18a5d67ae3114eae9154f54b49dc9222",
      "3d3997d4c3c64f2bacddaf40fd167501",
      "46e892265fe444cd85da47a69452b067",
      "7614f2fca9da4962b6b98c3e70986ca3",
      "df40cbb12d5844208781b76dab3dbd37",
      "4e31c9552b244ccca013e70113c2ec91",
      "b878066850284994b9bdd008b4a54376",
      "52401d3e0e6240178746a5a43e516285",
      "2fa0a82ade904ea59f425cc523e7ce1a",
      "35e177b4e8a84df695d9c2d765b8610b",
      "90607eaf7ac34cbc8fdfe69023156cc7",
      "b32e1a9f48c146afa4c1c16ea874ec4a",
      "e54a649e03814c39b66d2dddfe0100c5",
      "75954c740938444b83ca9ea261736e31",
      "898c2b3073af4980bd9762fc40465679",
      "4e4129ddb5f343f68ffc4f93f4d36573",
      "1c75a72d3ea3494e96c936b4a685bced",
      "77b4c250c3f549388805fd5af421dacd",
      "928a915b43eb43a3b293e70089b62f90",
      "74bf231c11ad45a69f9c9f91d4e0da6e",
      "a33584da08654868a5b2349791462637",
      "2cd036353b054b269bc79b845d07f7a7",
      "1d617c694fef404981c05431771606d3",
      "8af1b344f1b047ce82d8e578e94f99e0",
      "52008f3c3d6b46cfad409e02d83f9a0a",
      "0b405ed7cb0e44fd94f507c7e5b13862",
      "099123ae1b7b4204b8f35d0da23d35a7",
      "28f4839ca765491cb965ca20145df3c1",
      "36375adfb5514139ad75af34f5a21c1a",
      "7623dfe77901438a8a7abe6cf036e760",
      "3e12615f3f524e86990788be7031493d",
      "1f5f29e7678a446ea9120dc079216ec7",
      "2af7ddfc4e4749149069e8c84b28a632",
      "ce010b83d8984a9b95b7468f5e0e971d",
      "2273cf08251a41449e29af1b1b6e2c25",
      "0df2722f4a944941842508925b61e4f8",
      "28557d25658b4627b9d40df6ec384f7b",
      "4d008d295d934f09a6e932a9b5b7e5ea",
      "fdb94179fa2449ac835fdc111e555405",
      "dc305163b6e0414fbaedfd9879c226f5",
      "5ed88e5960584d1db0fbe24d80719aa2",
      "2bf55ce6685a4b71ab1f0347c1e392d1",
      "8095e4dde1b74235ad61e78e6797189c",
      "92fe7c0706e14ce5a8630377d340babf",
      "8bb06ca62f9c44ab840ce388bc542376",
      "ed772110bff243c7b4cee033a102cf55",
      "610e6bc249324ba0ace277d9d9209e8e",
      "f115dd4e13b945428dd0db09bd3d97e5",
      "a7aaba414359449e8ee6598f7656d043",
      "962d5017d79640d8a492b3f8a65e1cf6",
      "51e9940a6ce84bfeae66dc0d273f9f2b",
      "abb8de5471d24580aa270ce46c8eb816",
      "ef6f8be0fc6749009794401330d4ba80",
      "b8fb4c368a064f02bda50b551935ad3a",
      "dd25ac2204c44dc9a0a58027c2eca667",
      "03fe9104618649cc8eb866f9f92af18d",
      "23fb51259bfb46c3baf7e3be6d1764df",
      "662d051db0c3400f98972d7bb8d2fa14",
      "79b313dcfb5f425ab70d591cff0ce750",
      "0015ab4f67fd46dfb270029fa80e73a2",
      "0012b38c4c5f4c6186d320f6bdbd3d5d",
      "2bc15df1c54547d1a1c7f43aec40c76d",
      "2047a281c04c443fb6d3a77c29f5c045",
      "8517916e6ab64483a717093d69f77394",
      "71f6b227e8e74a4b978f1212d07c6dd8",
      "502829f21f334355bfe721192cb1a400",
      "0f963de4d7764ee7963e2704085fa62c",
      "afd6799e4ad94f1bb76e74b2ebb0c696",
      "877b4fd0b9a34552a685f210f58f8cbc",
      "d46f33fdc4bc446c909f84c943e228db",
      "df2afeae62df4e00885413a0c0ee11b0",
      "18b41c3cb4024a0bb335b7c028b4d22f",
      "eb2864fa04d74b7da2d703a1b3d25315",
      "590e427071e44a5e9a21843bd6792e50",
      "a8526f778a954a5ba403429df3b4cc77",
      "c137b2dd2f5e472da46d36994294d484",
      "66065f9a255640e2b2208877696a104b",
      "677a218124e54933809053867cfcd49e",
      "d9071b5f160447248fc5db61095cb4a4",
      "c9ed11e364fb4c5dae5b9687eda27be2",
      "d621cf1d251e4d6780098cbba13341be",
      "2222108444f34e88aeda8f9041d93aa1",
      "bcab436f655846bd9182cfd2e37ca151",
      "6e7365e760f9408daca557e7bb4c6281",
      "8f8c7e6c41e1445f8c703fdba49a9662",
      "0468b456f651495da41f55301fa3bbf2",
      "eb3050793e9447c4a80a03258d0986fc",
      "c5d04b29b9914af387727c1bc4aee0e6",
      "0c16f21965e94f5088222181159de978",
      "2081e8aa64ea4c6f93b59d3f2aa28406",
      "9d1d7ac691864775a70a01fd1ad9185e",
      "da9e795ea2084684a0282452b6751e3b",
      "22f8a4816f6340ceb45c9c252e163757",
      "e6c3fd28e0c84217a6d263e8b9fc2bbd",
      "26cdf2aa2aee42138c1e94648c23fd52",
      "363cfa0b14d246ecbcbe6e63744a8ae2",
      "8584fa57ffed4e2e9ff2ff58631b9a65",
      "7cfbf05881c24a279e26316780bdb0b4",
      "1d743993ab62461294b8cc8c0f369548",
      "d6e3ae5c196b4828a884d528977a0fb5",
      "ad172f5020084b0a9a0477fc3b980ac4",
      "af953d60add843db981a86d71a2a42fc",
      "863aab57494b488591d53a7167326a49",
      "7be53068f36148e090579f3155e1341b",
      "a6f2bf8419ed4fed86e4d2421474677d",
      "6878a1d012304fc78fc65a58501df96e",
      "7039fedb04c94f9cb0842b091ebeaa97",
      "7ad68dbeca23437ebe24dc0abc3e1d8e",
      "eeba2867e28a4579bdcf1e9dee6761b5",
      "f08f596b2f1045839b6df0872442271c",
      "d8a0359b56e0432ab69983f92aa0f6dc",
      "861fbdef6cec499d9e45a61bf0924e07",
      "52771933455a469ba85ddd21da5d4105",
      "a0910118c77d44d88efb5e514ac11fde",
      "0937a84ee2c846758b0212a7465679ae",
      "b7d98266e768451187fad778bb1ecbcb",
      "c64121f615e748639814c79e6f15da1b",
      "e191c9a7356c49478cb192f239cbff5f",
      "79474e88606141fba54f6702c9e06da5",
      "86bb3fc9168c490985379fe34acc6448",
      "ab1600ff92964f9e94ff50d7e9f8ccdd",
      "d3f1122fc4de467a8979b9ed4f10bd31",
      "84cbcbe2aa294bceb687de64337afcfa",
      "d5e689de12bb463299a8e93ee26fc0cb",
      "91fc420413fa4d17a6fe314c2238bfa1",
      "c84c91b63d984c859811ddb962825b32",
      "56dcca21d1ef467596b2e1cde848f000",
      "f282612daf184c52a78d516d7c87641a",
      "7c43ea70b62642c18f5136bb1e9301f4",
      "e730a7ac00e640438f34849496f05163",
      "347edc1ede024e939c52f559d7cf8cd3",
      "94c9f13f959548ef8966f3a4e683e677",
      "937457097c7b41468cd36ae18fc1b46f",
      "8b9caf3f60eb4d6f8ca86c211917fe8e",
      "58f4085c273841f0ba861e77f8b371c3",
      "d8e7aaea739c41a799001af977732dee",
      "e79b73e372ef435db60a96d7a03d398b",
      "28252b692a9647cab59a37f6c62e7a56",
      "29a6b15b857545b1a83d611a82d407df",
      "368505a01d014f428fe1743507b76826",
      "aae3840c66e843a59d118bda43029c37",
      "d86b3b55fb614018bcaabcc120bfc9cf",
      "cffb3776b722474c9f2ee3e947ee9fde",
      "cc1592a9ae8d472ab9e3cba9ccf4132e",
      "be840f7512ab4babb8d8bf4489b130ea",
      "3581ed1787144a63a7dfbdae0ff2a06b",
      "fb0360297aae4cf19c0f73c54a7868b2",
      "81d288ee61dc4dd08330393e47eac514",
      "bb3a03260cb5472f845837883b0ff573",
      "3ea2001d317c4648946335aa2c04a1ce",
      "4cca55bb03dc412e938d18de92318cab",
      "e0ae8c18b1c34452aa1c4fe51acaa54a",
      "91fe09759db64d549cd3f048a3da824b",
      "1f0464c44f8240c39c6f83dc75318206",
      "790180a05eed4f3c85c2af109c41fad3",
      "12bd886454234d138b6ef0ff481d2c98",
      "43863340881a47d192204e1d4069861e",
      "2ff6b05d16374d46af884736b6d87586",
      "726296deffd74584a4eb52c5ea52dd99",
      "600c0b718a274d4e8e58121d2cd97a42",
      "6d057ff8e6f34beb9813a1367a88092d",
      "6d1adfcd6d724425a94020d54ecf1d77",
      "3b0d2827767647bda43f52bf4bd31ea1",
      "0b2e919d25634f689bd52d281177783d",
      "5e2e416f9f044fa0906de2e3b4a6e460",
      "b4cff8984b104e0089f2adb81e328d42",
      "70784409f78246b9b7899c0199a3adef",
      "02244bf274dd44d5b79dead21b3b1a5c",
      "877c71b4652d43728177c322a75cbdb1",
      "4139d08ce3a541f78b13bb177d82e02d",
      "de60bc79e6fa4eddbdba4292fa44ce80",
      "c90bd0a483f847d7a9f163e3502ed264",
      "350af51e8d9a41febf0239b88b168998",
      "ea1364ec0aae4a1bbe693d91b2c73d64",
      "f949a658add346c8999086ee2998af6a",
      "b913b43f3c904b4098a80a8c89c6c113",
      "4830ab5929da4f93946deb70cddf437f",
      "f75d9915f3704c6d83797a0eb563fc51",
      "9e07ad454c4447ffbe0ee1d877d59660",
      "6d3e5546285e4adda089be621dabeb5c",
      "ef96fee0016c4e16b8d2ed7b6e0f0007",
      "caa6a9fcfa504da3b704b29a21ac99b2",
      "02916cdbc77d471ebf7af228cb5a900f",
      "be024a92c92b4a4f926f906579967a27",
      "9bdf51864e7b47468568eab0b28316d6",
      "70d9461e6c2c473a8b94a28e0da75054",
      "2b39a501d865403280c8fe2626439338",
      "f49ba786bb47451a993d7e7c5d0f4c8b",
      "9ca17ca7a4df46858380547760881bae",
      "92b394c165664fb4b4793f9cb3d710da",
      "bde943ee86864e9c8d8334a08784db49",
      "71c3120b7e0d42299f07076f015f4b11",
      "3f9fe66c5d95462faf165e22003e505d",
      "c4c784d819ce4c488b2dfbe45ac91680",
      "3040eb14024447afa0de48c1ef5b4fd7",
      "6c571afe8ba44b98b1e6fea3186b7020",
      "449550132f3643cbb7ff68e3cd2ac345",
      "7fa1b4cc510a432fa0f9ee5d886f7662",
      "1992efff6df24b8c8b8ffb42937339b3",
      "cf909e7e24a340ea9cd4e7acd3d612d9",
      "f0b4e9892f0d4b7e89090f822be2f83b",
      "c68510424bb2447fb3c5e70f76cad1d2",
      "f2e280974d4340d6afa8d059a2f29ea4",
      "5513b4a699ab4938a4b2d95cf5dd5011",
      "7ac8ea23d855470c9a09d4c2b95351f2",
      "18a71b143f884302be43d272b18c0c5f",
      "7fb046a74792446390dbd41cc865b8bb",
      "39f4a0652d224a22a5f52c467dfa25dc",
      "54d96e5ee53e4aef92abdd57c633e60b",
      "7b45a3ee23824cd79a88ee978d95fb2f",
      "9a83ac369776493a9caebc76f5fadbee",
      "cdd04e654f3544f7adbd60177312419c",
      "29d3f308db87422389513c7f3f210751",
      "e32fafe6189040288609b3731a6cdba3",
      "098d9ebf664c4b4587b345822ab82dee",
      "9ad1f5be93a648c1bc083972f69c7443",
      "1a1abbf339b74ecaa78a5bab7ae40d8a",
      "1ef1b4fe6c504780be41ac0461785d71",
      "7594e2f7c34d4b2aad767508630122d8",
      "6fa9ecf44c9b4c268ce7504863d4368d",
      "7bcca5c114f64ab4b01c644b1d84184f",
      "54aea9c5b3f646f9aa3af224b66b77bb",
      "c400a3c292ec48e6981b264b8293c933",
      "99cbdb17396b4c3fbd45cc9653c1ec96",
      "f7038153fc7f4add9a118c8f077d5688",
      "18333c4e67d04b16ba7ec0273e26c012",
      "fb31d42a67c24c108d2b52307d476560",
      "29f2a787c69f480b93660f9e1c778a73",
      "34e077a2a3f642399da5f42a5ab29fbe",
      "0782d67f86be4f79af4578ff6f9582cb",
      "e336790fa5b74a2cb646fd1493808a23",
      "630db27029d04782ae60a99e44aefb9e",
      "53605e9834d440cb9a7671dff3926b30",
      "44ba5d019045431b9c8f50d469ed5dfd",
      "342f2592eafb415bb6af9874ef50b75c",
      "600a9ba478d34b6792331a57176418ac",
      "b879a2ec0ce44346bef1d224f1d7fa13",
      "7244b944f7bb434e94f0703eacc4f3b1",
      "88409468f04746ab8d190888eb56b747",
      "f6a9400e71e94400a7150eed8e67fbbc",
      "fe6f68a4742a4dcf9ac6950a29f7c7d8",
      "cc3cc5f864284daf8a1f9428858b4bd7",
      "d0bd5d7bb9624c718ccc994a5b546a5b",
      "c2c820c773564cbfaff17d142df03b02",
      "6b5de96d014b48f2bd12001bdf0fd7a9",
      "3995b5261deb4971936112c285d21cc7",
      "42545e1a97a64dfea7fd07209223f42f",
      "ec23b8e63e1244b4a4bbf9f15b19356e",
      "505488c7bc7b479387f4565671867246",
      "3efb5bcdb6314ebca0cb7ecb8efa1c8b",
      "d165abfdc9a646fd9e3acd49f5bec4a2",
      "34c55043ff224005b560d72bb31d6c75",
      "cf0324e01ab74fa686d88acc790d551b",
      "2bb1af7bdba04b0188fff879a0de25b8",
      "d12188e0d20a4c27b92b0f2b1321b5fa",
      "acc0512c81394fe4af861f939e668273",
      "d3a5fe1d5c77478c80a48d2f70a5f428",
      "316bb584b2d54623b530b452b3148acd",
      "c2d7c06f5fc24b92a4455fb934ba2646",
      "e780ac9fcff14e45b879a5f5f8c3311b",
      "10975ebd611c4b5cb33a8451707efef2",
      "dbf44a1951c04e4588859c993b766cff",
      "4729a8d496a14180bdc8553bf21e3aae",
      "ef4af8a1558c4fa8aebd7bda701c42f9",
      "866eafb99c4c41f5ad266494653d69c0",
      "256bbbd1426d4d308d3aefe42e79ba71",
      "2a447607a29448a9b0f8992d67f7ce83",
      "356e0eb78b504690af29fabb28e00f9b",
      "e1aad834cc8d4942a7e2fbed5e9b5cad",
      "bc968dc115f045f9b05cc8fa3fc256dc",
      "ae7cdb2df9134b008a1d959d2043d27c",
      "572fc05647a046cd8363781d5c9addcc",
      "fe84999d733642a7bdca8ec2062eb621",
      "70fa3586573440f2b5002b233d338458"
     ]
    },
    "executionInfo": {
     "elapsed": 2305116,
     "status": "ok",
     "timestamp": 1743750294153,
     "user": {
      "displayName": "yu Wu",
      "userId": "12692660435918028293"
     },
     "user_tz": -60
    },
    "id": "U9lMB9DZeW-y",
    "outputId": "b67b00a4-013c-4f47-dd86-60058a237357"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fine-tuning and evaluating model: bert-base-uncased =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebb70cb104f4aa5aa0faf13f2e2ee3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bdff2e6f61b42b0b860fb00d25c66bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5693c382a12c4b5da9b009c18202e91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86c3a65304f4271bb9364c80de37270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1ef2851f69471d9b9486e724e6f656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6192' max='6192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6192/6192 06:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.081300</td>\n",
       "      <td>1.090498</td>\n",
       "      <td>0.608190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.862300</td>\n",
       "      <td>1.079334</td>\n",
       "      <td>0.672644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.309100</td>\n",
       "      <td>1.572705</td>\n",
       "      <td>0.674582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Performance for bert-base-uncased ===\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "         NEUTRAL       0.71      0.71      0.71      2017\n",
      "STRONGLYNEGATIVE       0.37      0.25      0.30        28\n",
      "STRONGLYPOSITIVE       0.37      0.33      0.35        76\n",
      "  WEAKLYNEGATIVE       0.51      0.50      0.50       440\n",
      "  WEAKLYPOSITIVE       0.69      0.71      0.70      1566\n",
      "\n",
      "        accuracy                           0.67      4127\n",
      "       macro avg       0.53      0.50      0.51      4127\n",
      "    weighted avg       0.67      0.67      0.67      4127\n",
      "\n",
      "Accuracy: 0.6745820208383814\n",
      "\n",
      "===== Fine-tuning and evaluating model: roberta-base =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0eb621b179b4d88b015e89589878d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7d1df1005a4119aaa194131c2c876e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf8f71b22f54633bc491f5f7ca7376e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5038868f27e943bebbd29a4cd72af0f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90607eaf7ac34cbc8fdfe69023156cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd036353b054b269bc79b845d07f7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6192' max='6192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6192/6192 06:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.061900</td>\n",
       "      <td>1.103679</td>\n",
       "      <td>0.639448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.884700</td>\n",
       "      <td>1.021320</td>\n",
       "      <td>0.659074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.613300</td>\n",
       "      <td>1.133411</td>\n",
       "      <td>0.670705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Performance for roberta-base ===\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "         NEUTRAL       0.75      0.65      0.70      2017\n",
      "STRONGLYNEGATIVE       0.37      0.25      0.30        28\n",
      "STRONGLYPOSITIVE       0.29      0.45      0.35        76\n",
      "  WEAKLYNEGATIVE       0.49      0.62      0.55       440\n",
      "  WEAKLYPOSITIVE       0.67      0.74      0.70      1566\n",
      "\n",
      "        accuracy                           0.67      4127\n",
      "       macro avg       0.52      0.54      0.52      4127\n",
      "    weighted avg       0.68      0.67      0.67      4127\n",
      "\n",
      "Accuracy: 0.6707051126726435\n",
      "\n",
      "===== Fine-tuning and evaluating model: distilbert-base-uncased =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af7ddfc4e4749149069e8c84b28a632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fe7c0706e14ce5a8630377d340babf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd25ac2204c44dc9a0a58027c2eca667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502829f21f334355bfe721192cb1a400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66065f9a255640e2b2208877696a104b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6192' max='6192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6192/6192 03:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.072500</td>\n",
       "      <td>1.076863</td>\n",
       "      <td>0.632905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.771900</td>\n",
       "      <td>1.122435</td>\n",
       "      <td>0.661255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.392500</td>\n",
       "      <td>1.515739</td>\n",
       "      <td>0.664405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Performance for distilbert-base-uncased ===\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "         NEUTRAL       0.71      0.69      0.70      2017\n",
      "STRONGLYNEGATIVE       0.19      0.11      0.14        28\n",
      "STRONGLYPOSITIVE       0.32      0.36      0.34        76\n",
      "  WEAKLYNEGATIVE       0.48      0.48      0.48       440\n",
      "  WEAKLYPOSITIVE       0.68      0.71      0.69      1566\n",
      "\n",
      "        accuracy                           0.66      4127\n",
      "       macro avg       0.48      0.47      0.47      4127\n",
      "    weighted avg       0.66      0.66      0.66      4127\n",
      "\n",
      "Accuracy: 0.6644051369033196\n",
      "\n",
      "===== Fine-tuning and evaluating model: xlnet-base-cased =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d04b29b9914af387727c1bc4aee0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d743993ab62461294b8cc8c0f369548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08f596b2f1045839b6df0872442271c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1600ff92964f9e94ff50d7e9f8ccdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/467M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6192' max='6192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6192/6192 07:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.250800</td>\n",
       "      <td>1.308497</td>\n",
       "      <td>0.513690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.135000</td>\n",
       "      <td>1.098060</td>\n",
       "      <td>0.629028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.796300</td>\n",
       "      <td>1.116230</td>\n",
       "      <td>0.652774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c9f13f959548ef8966f3a4e683e677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/467M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Performance for xlnet-base-cased ===\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "         NEUTRAL       0.72      0.66      0.69      2017\n",
      "STRONGLYNEGATIVE       0.54      0.25      0.34        28\n",
      "STRONGLYPOSITIVE       0.22      0.37      0.28        76\n",
      "  WEAKLYNEGATIVE       0.49      0.57      0.52       440\n",
      "  WEAKLYPOSITIVE       0.66      0.69      0.68      1566\n",
      "\n",
      "        accuracy                           0.65      4127\n",
      "       macro avg       0.53      0.51      0.50      4127\n",
      "    weighted avg       0.66      0.65      0.66      4127\n",
      "\n",
      "Accuracy: 0.6527744124061061\n",
      "\n",
      "===== Fine-tuning and evaluating model: google/electra-base-generator =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffb3776b722474c9f2ee3e947ee9fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0464c44f8240c39c6f83dc75318206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2e416f9f044fa0906de2e3b4a6e460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b913b43f3c904b4098a80a8c89c6c113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b39a501d865403280c8fe2626439338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-generator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6192' max='6192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6192/6192 04:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.110500</td>\n",
       "      <td>1.071633</td>\n",
       "      <td>0.590986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.992300</td>\n",
       "      <td>0.980860</td>\n",
       "      <td>0.650351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.662800</td>\n",
       "      <td>1.056624</td>\n",
       "      <td>0.656409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa1b4cc510a432fa0f9ee5d886f7662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/135M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Performance for google/electra-base-generator ===\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "         NEUTRAL       0.72      0.66      0.69      2017\n",
      "STRONGLYNEGATIVE       0.47      0.29      0.36        28\n",
      "STRONGLYPOSITIVE       0.27      0.33      0.29        76\n",
      "  WEAKLYNEGATIVE       0.46      0.57      0.51       440\n",
      "  WEAKLYPOSITIVE       0.67      0.70      0.69      1566\n",
      "\n",
      "        accuracy                           0.66      4127\n",
      "       macro avg       0.52      0.51      0.51      4127\n",
      "    weighted avg       0.67      0.66      0.66      4127\n",
      "\n",
      "Accuracy: 0.6564090138114853\n",
      "\n",
      "===== Fine-tuning and evaluating model: vinai/bertweet-base =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d96e5ee53e4aef92abdd57c633e60b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/558 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa9ecf44c9b4c268ce7504863d4368d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/843k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e336790fa5b74a2cb646fd1493808a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bpe.codes:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3cc5f864284daf8a1f9428858b4bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0324e01ab74fa686d88acc790d551b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/543M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6192' max='6192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6192/6192 06:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.018700</td>\n",
       "      <td>1.036478</td>\n",
       "      <td>0.628544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.991500</td>\n",
       "      <td>0.952301</td>\n",
       "      <td>0.669009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.480100</td>\n",
       "      <td>1.154225</td>\n",
       "      <td>0.675794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4af8a1558c4fa8aebd7bda701c42f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/543M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Performance for vinai/bertweet-base ===\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "         NEUTRAL       0.75      0.65      0.70      2017\n",
      "STRONGLYNEGATIVE       0.33      0.36      0.34        28\n",
      "STRONGLYPOSITIVE       0.37      0.43      0.40        76\n",
      "  WEAKLYNEGATIVE       0.49      0.57      0.53       440\n",
      "  WEAKLYPOSITIVE       0.68      0.75      0.71      1566\n",
      "\n",
      "        accuracy                           0.68      4127\n",
      "       macro avg       0.52      0.55      0.54      4127\n",
      "    weighted avg       0.69      0.68      0.68      4127\n",
      "\n",
      "Accuracy: 0.6757935546401744\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"  # Disable wandb\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Dataset Splitting, Label Encoding, and Evaluation Metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "\n",
    "# Includes various pretrained models and utilities from Hugging Face Transformers\n",
    "from transformers import (\n",
    "    BertTokenizer, BertForSequenceClassification,\n",
    "    RobertaTokenizer, RobertaForSequenceClassification,\n",
    "    DistilBertTokenizer, DistilBertForSequenceClassification,\n",
    "    XLNetTokenizer, XLNetForSequenceClassification,\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer\n",
    ")\n",
    "# Neural network submodules in PyTorch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============== 1. Data Loading and Cleaning ==============\n",
    "df = pd.read_csv(\n",
    "    \"SemEval2017-task4-dev.subtask-CE.english.INPUT.txt\",\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=['id', 'topic', 'label_num', 'tweet_raw'],\n",
    ")\n",
    "\n",
    "# Mapping Between Numeric and String Labels\n",
    "label_map = {\n",
    "    -2: \"STRONGLYNEGATIVE\",\n",
    "    -1: \"WEAKLYNEGATIVE\",\n",
    "     0: \"NEUTRAL\",\n",
    "     1: \"WEAKLYPOSITIVE\",\n",
    "     2: \"STRONGLYPOSITIVE\"\n",
    "}\n",
    "df['label'] = df['label_num'].map(label_map)\n",
    "\n",
    "# To ensure fairness and comparability, the data cleaning procedures applied to the BERT family of models\n",
    "# are kept consistent with those used for the RNN, BiLSTM, and CNN models\n",
    "def basic_text_cleaning(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9(),!?\\'`]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip() # Replace multiple spaces with a single space and strip leading/trailing whitespace\n",
    "    return text\n",
    "\n",
    "df['tweet'] = df['tweet_raw'].astype(str).apply(basic_text_cleaning) # Perform type conversion\n",
    "\n",
    "# Concatenate the topic and tweet into a single input sequence for BERT\n",
    "df['input_text'] = df.apply(lambda row: f\"[TOPIC] {row['topic']} [SEP] {row['tweet']}\", axis=1)\n",
    "\n",
    "# ============== 2. Augmentation Using a Sentiment Lexicon ==============\n",
    "senti_lexicon = {\n",
    "    \"love\": 2, \"like\": 1, \"good\": 1, \"hate\": -2, \"bad\": -1, \"horrible\": -2\n",
    "}\n",
    "def lexicon_score(sentence):\n",
    "    words = sentence.lower().split() # Convert sentences to lowercase and compare word by word\n",
    "    score = 0\n",
    "    for w in words:\n",
    "        if w in senti_lexicon:\n",
    "            score += senti_lexicon[w]\n",
    "    return score\n",
    "\n",
    "df['lexicon_score'] = df['tweet'].apply(lexicon_score) # Store the scores in df['lexicon_score']\n",
    "\n",
    "# ============== 3. Data splitting and handling of class imbalance ==============\n",
    "le = LabelEncoder()\n",
    "df['label_id'] = le.fit_transform(df['label'])  # Convert to the range 0–4\n",
    "\n",
    "# Use stratified sampling to ensure similar label distributions in the training and test sets\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df['label_id'] # Set a random seed to ensure reproducibility\n",
    ")\n",
    "\n",
    "# Compute class weights for use in weighted cross-entropy loss\n",
    "# Assign higher weights to rare classes\n",
    "train_labels_array = train_df['label_id'].to_numpy()\n",
    "class_counts = Counter(train_labels_array)\n",
    "num_samples = len(train_labels_array)\n",
    "num_classes = len(class_counts)\n",
    "weights = [num_samples / (num_classes * class_counts[i]) for i in range(num_classes)]\n",
    "class_weights = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "# ============== 4. Construct the Dataset ==============\n",
    "# Use the tokenizer to tokenize the input text, convert it to token IDs,\n",
    "# truncate or pad to max_len, and generate the corresponding attention_mask\n",
    "class BERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len # Maximum length per text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):  # Tokenize the input text, convert it to token IDs, and apply truncation or padding to max_len\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True, # Automatically add [CLS] and [SEP] tokens\n",
    "            max_length=self.max_len, # Set the maximum sequence length\n",
    "            padding='max_length',  # Pad sequences shorter than max_len\n",
    "            truncation=True,     # Truncate sequences that exceed max_len\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze()  # Return as a dictionary\n",
    "        attention_mask = encoding['attention_mask'].squeeze() # Remove extra dimensions\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,      # Token ID\n",
    "            'attention_mask': attention_mask, # Attention mask\n",
    "            'labels': torch.tensor(label, dtype=torch.long) # Convert labels to LongTensor type\n",
    "        }\n",
    "\n",
    "# Convert to Python lists for constructing a custom Dataset\n",
    "train_texts = train_df['input_text'].tolist()\n",
    "train_labels = train_df['label_id'].tolist()\n",
    "test_texts = test_df['input_text'].tolist()\n",
    "test_labels = test_df['label_id'].tolist()\n",
    "\n",
    "# ============== 5. Custom Trainer with Weighted Cross-Entropy Loss ==============\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Because additional arguments (e.g., num_items_in_batch) may be passed to Trainer during execution,\n",
    "        **kwargs is included to prevent errors\n",
    "        \"\"\"\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"}) # Perform forward propagation\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Using class_weights（Alternatively, use the ordinary loss_fct = nn.CrossEntropyLoss()）\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights.to(logits.device)) # Compute the loss\n",
    "        loss = loss_fct(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# ============== 6. Training Configuration (TrainingArguments) ==============\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./checkpoints',\n",
    "    num_train_epochs=3,       # Train for 3 epochs\n",
    "    per_device_train_batch_size=8, # Set the batch size to 8 per GPU/CPU during training and validation\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,  # Log every 50 steps\n",
    "    do_eval=True,\n",
    "    report_to=\"none\",  # Turn off wandb logging\n",
    ")\n",
    "\n",
    "# ============== 7. Train each model sequentially and output its test performance separately ==============\n",
    "# Save both the model and tokenizer for later use in ensemble evaluation\n",
    "model_names = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"roberta-base\",\n",
    "    \"distilbert-base-uncased\",\n",
    "    \"xlnet-base-cased\",\n",
    "    \"google/electra-base-generator\",\n",
    "    # The most suitable BERT model for recognizing Twitter tweets\n",
    "    \"vinai/bertweet-base\"\n",
    "]\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "train_dataset = BERTDataset(train_texts, train_labels, None, max_len=128)\n",
    "test_dataset  = BERTDataset(test_texts,  test_labels,  None, max_len=128)\n",
    "\n",
    "all_models = []\n",
    "all_tokenizers = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n===== Fine-tuning and evaluating model: {model_name} =====\")\n",
    "\n",
    "    # 1) Load tokenizer & model\n",
    "    # BERTweet follows the same architecture as RoBERTa and can be used via AutoTokenizer and AutoModel\n",
    "    if \"roberta\" in model_name.lower():\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "    elif \"distilbert\" in model_name.lower():\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "    elif \"xlnet\" in model_name.lower():\n",
    "        tokenizer = XLNetTokenizer.from_pretrained(model_name)\n",
    "        model = XLNetForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "    elif \"electra\" in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "    elif \"bertweet\" in model_name.lower():\n",
    "        # BERTweet is typically based on the RoBERTa architecture\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "    else:\n",
    "        # Default BERT\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        model = BertForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "\n",
    "    # 2) Update the tokenizer used in the dataset\n",
    "    train_dataset.tokenizer = tokenizer\n",
    "    test_dataset.tokenizer  = tokenizer\n",
    "\n",
    "    # 3) Define the Trainer\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,      # Loaded pretrained model\n",
    "        args=training_args,  # Training Parameters\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # 4) Training\n",
    "    trainer.train()\n",
    "\n",
    "    # 5) Test set prediction with a single model\n",
    "    pred_output = trainer.predict(test_dataset)\n",
    "    predictions = pred_output.predictions\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    test_labels_true = test_df['label_id'].tolist()\n",
    "\n",
    "    # 6) Display evaluation for the single model\n",
    "    print(f\"=== Test Performance for {model_name} ===\")\n",
    "    print(classification_report(test_labels_true, preds, target_names=le.classes_))\n",
    "    acc = accuracy_score(test_labels_true, preds)\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    # 7) Save the model and tokenizer to a list for later use in ensemble\n",
    "    all_models.append(model)\n",
    "    all_tokenizers.append(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 262506,
     "status": "ok",
     "timestamp": 1743750556661,
     "user": {
      "displayName": "yu Wu",
      "userId": "12692660435918028293"
     },
     "user_tz": -60
    },
    "id": "uuHQVHsUeo4r",
    "outputId": "f7f1b061-6949-4efa-fcce-855847bf8a3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Ensemble (logits average) on Test Set =====\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "         NEUTRAL       0.74      0.70      0.72      2017\n",
      "STRONGLYNEGATIVE       0.36      0.18      0.24        28\n",
      "STRONGLYPOSITIVE       0.32      0.34      0.33        76\n",
      "  WEAKLYNEGATIVE       0.52      0.56      0.54       440\n",
      "  WEAKLYPOSITIVE       0.69      0.74      0.72      1566\n",
      "\n",
      "        accuracy                           0.69      4127\n",
      "       macro avg       0.53      0.50      0.51      4127\n",
      "    weighted avg       0.69      0.69      0.69      4127\n",
      "\n",
      "Ensemble Accuracy: 0.6866973588563121\n"
     ]
    }
   ],
   "source": [
    "# ============== 8. Perform ensemble by averaging logits across models ==============\n",
    "def predict_ensemble(texts, max_len=128):\n",
    "    # Prefer using GPU acceleration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    for m in all_models:\n",
    "        m.to(device)\n",
    "        m.eval() # Set the model to evaluation mode\n",
    "\n",
    "# Initialize an empty list to store predictions for all input texts\n",
    "    preds_ens = []\n",
    "    for text in texts:\n",
    "        logits_sum = None\n",
    "        for tkn, mdl in zip(all_tokenizers, all_models):\n",
    "            inputs = tkn(\n",
    "                text,\n",
    "                return_tensors='pt',\n",
    "                max_length=max_len,\n",
    "                truncation=True,\n",
    "                padding='max_length'\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                out = mdl(**inputs)  # Forward propagation\n",
    "                logits = out.logits.detach().cpu().numpy()\n",
    "            if logits_sum is None:  # Initialize with the logits from the first model\n",
    "                logits_sum = logits\n",
    "            else:\n",
    "                logits_sum += logits\n",
    "\n",
    "        # Compute the mean\n",
    "        ensemble_logits = logits_sum / len(all_models)\n",
    "        # argmax\n",
    "        pred_label_id = np.argmax(ensemble_logits, axis=1)[0]\n",
    "        preds_ens.append(pred_label_id)\n",
    "\n",
    "    return preds_ens\n",
    "\n",
    "print(\"\\n===== Ensemble (logits average) on Test Set =====\")\n",
    "test_preds_ens = predict_ensemble(test_texts)\n",
    "test_labels_true = test_df['label_id'].tolist()\n",
    "print(classification_report(test_labels_true, test_preds_ens, target_names=le.classes_))\n",
    "acc_ens = accuracy_score(test_labels_true, test_preds_ens)\n",
    "print(\"Ensemble Accuracy:\", acc_ens)\n",
    "\n",
    "# Convert predictions from numeric labels back to text\n",
    "# ensemble_pred_labels_str = le.inverse_transform(test_preds_ens)\n",
    "# print(\"Sample ensemble predictions:\", ensemble_pred_labels_str[:10])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPOYV8w4Vwyq4pGbOUmVzwf",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
