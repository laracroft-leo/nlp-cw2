{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPjL8zI196oD7dZQUmMflt9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fvtic9sFdfti"},"outputs":[],"source":["import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\"  # Disable wandb\n","import re\n","import torch\n","import pandas as pd\n","import numpy as np\n","\n","# Dataset Splitting, Label Encoding, and Evaluation Metrics\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, accuracy_score\n","from sklearn.preprocessing import LabelEncoder\n","from collections import Counter\n","\n","# Includes various pretrained models and utilities from Hugging Face Transformers\n","from transformers import (\n","    BertTokenizer, BertForSequenceClassification,\n","    RobertaTokenizer, RobertaForSequenceClassification,\n","    DistilBertTokenizer, DistilBertForSequenceClassification,\n","    XLNetTokenizer, XLNetForSequenceClassification,\n","    AutoTokenizer, AutoModelForSequenceClassification,\n","    TrainingArguments, Trainer\n",")\n","# Neural network submodules in PyTorch\n","import torch.nn as nn\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# ============== 1. Data Loading and Cleaning ==============\n","df = pd.read_csv(\n","    \"SemEval2017-task4-dev.subtask-CE.english.INPUT.txt\",\n","    sep='\\t',\n","    header=None,\n","    names=['id', 'topic', 'label_num', 'tweet_raw'],\n",")\n","\n","# Mapping Between Numeric and String Labels\n","label_map = {\n","    -2: \"STRONGLYNEGATIVE\",\n","    -1: \"WEAKLYNEGATIVE\",\n","     0: \"NEUTRAL\",\n","     1: \"WEAKLYPOSITIVE\",\n","     2: \"STRONGLYPOSITIVE\"\n","}\n","df['label'] = df['label_num'].map(label_map)\n","\n","# To ensure fairness and comparability, the data cleaning procedures applied to the BERT family of models\n","# are kept consistent with those used for the RNN, BiLSTM, and CNN models\n","def basic_text_cleaning(text):\n","    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n","    text = re.sub(r\"@\\w+\", \"\", text)\n","    text = re.sub(r\"#\", \"\", text)\n","    text = re.sub(r\"[^A-Za-z0-9(),!?\\'`]\", \" \", text)\n","    text = re.sub(r\"\\s+\", \" \", text).strip() # Replace multiple spaces with a single space and strip leading/trailing whitespace\n","    return text\n","\n","df['tweet'] = df['tweet_raw'].astype(str).apply(basic_text_cleaning) # Perform type conversion\n","\n","# Concatenate the topic and tweet into a single input sequence for BERT\n","df['input_text'] = df.apply(lambda row: f\"[TOPIC] {row['topic']} [SEP] {row['tweet']}\", axis=1)\n","\n","# ============== 2. Augmentation Using a Sentiment Lexicon ==============\n","senti_lexicon = {\n","    \"love\": 2, \"like\": 1, \"good\": 1, \"hate\": -2, \"bad\": -1, \"horrible\": -2\n","}\n","def lexicon_score(sentence):\n","    words = sentence.lower().split() # Convert sentences to lowercase and compare word by word\n","    score = 0\n","    for w in words:\n","        if w in senti_lexicon:\n","            score += senti_lexicon[w]\n","    return score\n","\n","df['lexicon_score'] = df['tweet'].apply(lexicon_score) # Store the scores in df['lexicon_score']\n","\n","# ============== 3. Data splitting and handling of class imbalance ==============\n","le = LabelEncoder()\n","df['label_id'] = le.fit_transform(df['label'])  # Convert to the range 0–4\n","\n","# Use stratified sampling to ensure similar label distributions in the training and test sets\n","train_df, test_df = train_test_split(\n","    df, test_size=0.2, random_state=42, stratify=df['label_id'] # Set a random seed to ensure reproducibility\n",")\n","\n","# Compute class weights for use in weighted cross-entropy loss\n","# Assign higher weights to rare classes\n","train_labels_array = train_df['label_id'].to_numpy()\n","class_counts = Counter(train_labels_array)\n","num_samples = len(train_labels_array)\n","num_classes = len(class_counts)\n","weights = [num_samples / (num_classes * class_counts[i]) for i in range(num_classes)]\n","class_weights = torch.tensor(weights, dtype=torch.float)\n","\n","# ============== 4. Construct the Dataset ==============\n","# Use the tokenizer to tokenize the input text, convert it to token IDs,\n","# truncate or pad to max_len, and generate the corresponding attention_mask\n","class BERTDataset(torch.utils.data.Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len=128):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len # Maximum length per text\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):  # Tokenize the input text, convert it to token IDs, and apply truncation or padding to max_len\n","        text = self.texts[idx]\n","        label = self.labels[idx]\n","\n","        encoding = self.tokenizer(\n","            text,\n","            add_special_tokens=True, # Automatically add [CLS] and [SEP] tokens\n","            max_length=self.max_len, # Set the maximum sequence length\n","            padding='max_length',  # Pad sequences shorter than max_len\n","            truncation=True,     # Truncate sequences that exceed max_len\n","            return_tensors='pt'\n","        )\n","        input_ids = encoding['input_ids'].squeeze()  # Return as a dictionary\n","        attention_mask = encoding['attention_mask'].squeeze() # Remove extra dimensions\n","\n","        return {\n","            'input_ids': input_ids,      # Token ID\n","            'attention_mask': attention_mask, # Attention mask\n","            'labels': torch.tensor(label, dtype=torch.long) # Convert labels to LongTensor type\n","        }\n","\n","# Convert to Python lists for constructing a custom Dataset\n","train_texts = train_df['input_text'].tolist()\n","train_labels = train_df['label_id'].tolist()\n","test_texts = test_df['input_text'].tolist()\n","test_labels = test_df['label_id'].tolist()\n","\n","# ============== 5. Custom Trainer with Weighted Cross-Entropy Loss ==============\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n","        \"\"\"\n","        Because additional arguments (e.g., num_items_in_batch) may be passed to Trainer during execution,\n","        **kwargs is included to prevent errors\n","        \"\"\"\n","        labels = inputs.get(\"labels\")\n","        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"}) # Perform forward propagation\n","        logits = outputs.logits\n","\n","        # Using class_weights（Alternatively, use the ordinary loss_fct = nn.CrossEntropyLoss()）\n","        loss_fct = nn.CrossEntropyLoss(weight=class_weights.to(logits.device)) # Compute the loss\n","        loss = loss_fct(logits, labels)\n","\n","        return (loss, outputs) if return_outputs else loss\n","\n","# ============== 6. Training Configuration (TrainingArguments) ==============\n","training_args = TrainingArguments(\n","    output_dir='./checkpoints',\n","    num_train_epochs=3,       # Train for 3 epochs\n","    per_device_train_batch_size=8, # Set the batch size to 8 per GPU/CPU during training and validation\n","    per_device_eval_batch_size=8,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_dir='./logs',\n","    logging_steps=50,  # Log every 50 steps\n","    do_eval=True,\n","    report_to=\"none\",  # Turn off wandb logging\n",")\n","\n","# ============== 7. Train each model sequentially and output its test performance separately ==============\n","# Save both the model and tokenizer for later use in ensemble evaluation\n","model_names = [\n","    \"bert-base-uncased\",\n","    \"roberta-base\",\n","    \"distilbert-base-uncased\",\n","    \"xlnet-base-cased\",\n","    \"google/electra-base-generator\",\n","    # The most suitable BERT model for recognizing Twitter tweets\n","    \"vinai/bertweet-base\"\n","]\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    preds = np.argmax(logits, axis=1)\n","    acc = accuracy_score(labels, preds)\n","    return {\"accuracy\": acc}\n","\n","train_dataset = BERTDataset(train_texts, train_labels, None, max_len=128)\n","test_dataset  = BERTDataset(test_texts,  test_labels,  None, max_len=128)\n","\n","all_models = []\n","all_tokenizers = []\n","\n","for model_name in model_names:\n","    print(f\"\\n===== Fine-tuning and evaluating model: {model_name} =====\")\n","\n","    # 1) Load tokenizer & model\n","    # BERTweet follows the same architecture as RoBERTa and can be used via AutoTokenizer and AutoModel\n","    if \"roberta\" in model_name.lower():\n","        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","        model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=5)\n","    elif \"distilbert\" in model_name.lower():\n","        tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n","        model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=5)\n","    elif \"xlnet\" in model_name.lower():\n","        tokenizer = XLNetTokenizer.from_pretrained(model_name)\n","        model = XLNetForSequenceClassification.from_pretrained(model_name, num_labels=5)\n","    elif \"electra\" in model_name.lower():\n","        tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n","    elif \"bertweet\" in model_name.lower():\n","        # BERTweet is typically based on the RoBERTa architecture\n","        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n","        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n","    else:\n","        # Default BERT\n","        tokenizer = BertTokenizer.from_pretrained(model_name)\n","        model = BertForSequenceClassification.from_pretrained(model_name, num_labels=5)\n","\n","    # 2) Update the tokenizer used in the dataset\n","    train_dataset.tokenizer = tokenizer\n","    test_dataset.tokenizer  = tokenizer\n","\n","    # 3) Define the Trainer\n","    trainer = CustomTrainer(\n","        model=model,      # Loaded pretrained model\n","        args=training_args,  # Training Parameters\n","        train_dataset=train_dataset,\n","        eval_dataset=test_dataset,\n","        compute_metrics=compute_metrics\n","    )\n","\n","    # 4) Training\n","    trainer.train()\n","\n","    # 5) Test set prediction with a single model\n","    pred_output = trainer.predict(test_dataset)\n","    predictions = pred_output.predictions\n","    preds = np.argmax(predictions, axis=1)\n","    test_labels_true = test_df['label_id'].tolist()\n","\n","    # 6) Display evaluation for the single model\n","    print(f\"=== Test Performance for {model_name} ===\")\n","    print(classification_report(test_labels_true, preds, target_names=le.classes_))\n","    acc = accuracy_score(test_labels_true, preds)\n","    print(\"Accuracy:\", acc)\n","\n","    # 7) Save the model and tokenizer to a list for later use in ensemble\n","    all_models.append(model)\n","    all_tokenizers.append(tokenizer)"]}]}